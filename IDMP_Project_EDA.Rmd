---
title: "EDA"
author: "Mudit Bhartia"
date: "12/5/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      cache = FALSE,
                      fig.align = 'center',
                      warning = FALSE,
                      message = FALSE)

```

```{r }


library('ggplot2') # visualization
library('ggthemes') # visualization
library('scales') # visualization
library('grid') # visualisation
library('gridExtra') # visualisation
library('corrplot') # visualisation
library('ggrepel') # visualisation
library('RColorBrewer') # visualisation
library('data.table') # data manipulation
library('dplyr') # data manipulation
library('readr') # data input
library('tibble') # data wrangling
library('tidyr') # data wrangling
library('lazyeval') # data wrangling
library('broom') # data wrangling
library('stringr') # string manipulation
library('purrr') # string manipulation
library('forcats') # factor manipulation
library('lubridate') # date and time
library('gridExtra')





```

## Loading Data



```{r}

train <- read_csv('/Users/muditbhartia/Desktop/CS5110/IDMP Project /dataset/web-traffic-time-series-forecasting/train_1.csv')

key <- read_csv('/Users/muditbhartia/Desktop/CS5110/IDMP Project /dataset/web-traffic-time-series-forecasting/key_1.csv')

#train_2 <- read_csv('/Users/muditbhartia/Desktop/CS5110/IDMP Project /dataset/web-traffic-time-series-forecasting/train_2.csv')

head(train,5)


```





## File structure and content

* Dimensions of the train data set:


```{r}

c(ncol(train),nrow(train))
train %>% colnames() %>% head(5)
train %>% select(Page) %>% head(5)

```

The data is originally structured so that 550 dates refer to a column each 
and the 145k article nanes are stored in the additional Page column


* Dimensions of the key data set:

```{r}

glimpse(key)


```

The key data contains a unique alpha-numerical ID for each Page and Date combination, which is the reason for the relatively large file size.


* Missing Values

```{r}

sum(is.na(train))/(ncol(train)*nrow(train))


```

There are about 8% of missing values in this data set, which is not trivial.
We will need to take them into account in our analysis.


## Data transformation

To make the training data easier to handle we split it into two part: the 
article information (from the Page column) and the time series data (tdates)
from the date columns. We briefly separate the article information into data
from wikipedia, wikimedia, and mediawiki due to the different formatting of 
the Page names. After that, we rejoin all article information into a common 
data set (tpages).

```{r}

tdates <- train %>% select(-Page)

foo <- train %>% select(Page) %>% rownames_to_column()
mediawiki <- foo %>% filter(str_detect(Page, "mediawiki"))
wikimedia <- foo %>% filter(str_detect(Page, "wikimedia"))
wikipedia <- foo %>% filter(str_detect(Page, "wikipedia")) %>% 
  filter(!str_detect(Page, "wikimedia")) %>%
  filter(!str_detect(Page, "mediawiki"))

wikipedia <- wikipedia %>%
  separate(Page, into = c("foo", "bar"), sep = ".wikipedia.org_") %>%
  separate(foo, into = c("article", "locale"), sep = -3) %>%
  separate(bar, into = c("access", "agent"), sep = "_") %>%
  mutate(locale = str_sub(locale,2,3))

wikimedia <- wikimedia %>%
  separate(Page, into = c("article", "bar"), sep = "_commons.wikimedia.org_") %>%
  separate(bar, into = c("access", "agent"), sep = "_") %>%
  add_column(locale = "wikmed")

mediawiki <- mediawiki %>%
  separate(Page, into = c("article", "bar"), sep = "_www.mediawiki.org_") %>%
  separate(bar, into = c("access", "agent"), sep = "_") %>%
  add_column(locale = "medwik")

tpages <- wikipedia %>%
  full_join(wikimedia, by = c("rowname", "article", "locale", "access", "agent")) %>%
  full_join(mediawiki, by = c("rowname", "article", "locale", "access", "agent"))

sample_n(tpages, size = 10)
#head(tdates,5)

```


Now we can search for certain Page subjects and filter their meta parameters:

```{r}

tpages %>% filter(str_detect(article, "The_Beatle")) %>%
  filter(access == "all-access") %>%
  filter(agent == "all-agents")

```

## Time series extraction

In order to plot the time series data we use a helper function that allows
us to extract the time series for a specified row number. (The normalised 
version is to facilitate the comparision between multiple time series 
curves, to correct for large differences in view count.)


```{r}

extract_ts <- function(rownr){
  tdates %>%
    rownames_to_column %>% 
    filter(rowname == as.character(rownr)) %>% 
    gather(dates, value, -rowname) %>% 
    spread(rowname, value) %>%
    mutate(dates = ymd(dates)) %>% 
    rename(views = as.character(rownr))
}

extract_ts_nrm <- function(rownr){
  tdates %>%
    rownames_to_column %>% 
    filter(rowname == as.character(rownr)) %>% 
    gather(dates, value, -rowname) %>% 
    spread(rowname, value) %>%
    mutate(dates = ymd(dates)) %>% 
    rename(views = as.character(rownr)) %>% 
    mutate(views = views/mean(views))
}



```


A custom-made plotting function allows us to visualise each time series and extract its meta data:

```{r}

plot_rownr <- function(rownr){
  art <- tpages %>% filter(rowname == rownr) %>% .$article
  loc <- tpages %>% filter(rowname == rownr) %>% .$locale
  acc <- tpages %>% filter(rowname == rownr) %>% .$access
  extract_ts(rownr) %>%
    ggplot(aes(dates, views)) +
    geom_line() +
    geom_smooth(method = "loess", color = "blue", span = 1/5) +
    labs(title = str_c(art, " - ", loc, " - ", acc))
}

plot_rownr_log <- function(rownr){
  art <- tpages %>% filter(rowname == rownr) %>% .$article
  loc <- tpages %>% filter(rowname == rownr) %>% .$locale
  acc <- tpages %>% filter(rowname == rownr) %>% .$access
  extract_ts_nrm(rownr) %>%
    ggplot(aes(dates, views)) +
    geom_line() +
    geom_smooth(method = "loess", color = "blue", span = 1/5) +
    labs(title = str_c(art, " - ", loc, " - ", acc)) +
    scale_y_log10() + labs(y = "log views")
}

plot_rownr_zoom <- function(rownr, start, end){
  art <- tpages %>% filter(rowname == rownr) %>% .$article
  loc <- tpages %>% filter(rowname == rownr) %>% .$locale
  acc <- tpages %>% filter(rowname == rownr) %>% .$access
  extract_ts(rownr) %>%
    filter(dates > ymd(start) & dates <= ymd(end)) %>%
    ggplot(aes(dates, views)) +
    geom_line() +
    geom_smooth(method = "loess", color = "blue", span = 1/5) +
    coord_cartesian(xlim = ymd(c(start,end))) +  
    labs(title = str_c(art, " - ", loc, " - ", acc))
}

```


```{r}
plot_rownr(11214)

```


In addition, with the help of the extractor tool we define a function that
re-connects the Page information to the corresponding time series and plots 
this curve according to our specification on article name, access type, and 
agent for all the available languages:


```{r}

plot_names <- function(art, acc, ag){

  pick <- tpages %>% filter(str_detect(article, art)) %>%
    filter(access == acc) %>%
    filter(agent == ag)
  pick_nr <- pick %>% .$rowname
  pick_loc <- pick %>% .$locale

  tdat <- extract_ts(pick_nr[1]) %>%
    mutate(loc = pick_loc[1])

  for (i in seq(2,length(pick))){
    foo <- extract_ts(pick_nr[i]) %>%
    mutate(loc = pick_loc[i])
    tdat <- bind_rows(tdat,foo)
  }

  plt <- tdat %>%
    ggplot(aes(dates, views, color = loc)) +
    geom_line() + 
    labs(title = str_c(art, "  -  ", acc, "  -  ", ag))

  print(plt)
}

plot_names_nrm <- function(art, acc, ag){

  pick <- tpages %>% filter(str_detect(article, art)) %>%
    filter(access == acc) %>%
    filter(agent == ag)
  pick_nr <- pick %>% .$rowname
  pick_loc <- pick %>% .$locale

  tdat <- extract_ts_nrm(pick_nr[1]) %>%
    mutate(loc = pick_loc[1])

  for (i in seq(2,length(pick))){
    foo <- extract_ts_nrm(pick_nr[i]) %>%
    mutate(loc = pick_loc[i])
    tdat <- bind_rows(tdat,foo)
  }

  plt <- tdat %>%
    ggplot(aes(dates, views, color = loc)) +
    geom_line() + 
    labs(title = str_c(art, "  -  ", acc, "  -  ", ag)) +
    scale_y_log10() + labs(y = "log views")

  print(plt)
}


```



```{r}

plot_names("The_Beatles", "all-access", "all-agents")

```


## Summary parameter extraction

In the next step we will have a more global look at the population parameters of our training time series data. Also here, we will start with the wikipedia data. The idea behind this approach is to probe the parameter space of the time series information along certain key metrics and to identify extreme observations that could break our forecasting strategies.

### Projects data overview

How the different meta-parameters are distributed:

```{r}

p1 <- tpages %>% 
  ggplot(aes(agent)) + geom_bar(fill = "red")
p2 <- tpages %>% 
  ggplot(aes(access)) + geom_bar(fill = "red")
p3 <- tpages %>% 
  ggplot(aes(locale, fill = locale)) + geom_bar() + theme(legend.position = "none")

layout <- matrix(c(1,2,3,3),2,2,byrow=TRUE)
grid.arrange(p1, p2, p3)

```
### Insights

* We find that our wikipedia data includes 7 languages: German, English, 
Spanish, French, Japanese, Russian, and Chinese. All of those are more 
frequent than the mediawiki and wikimedia pages. Mobile sites are slightly 
more frequent than desktop ones.


## Basic time series parameters

We start with a basic set of parameters: mean, standard deviation,
amplitude, and a the slope of a naive linear fit. This is our extraction 
function:

```{r}

params_ts1 <- function(rownr){
  foo <- tdates %>%
    filter_((interp(~x == row_number(), .values = list(x = rownr)))) %>%
    rownames_to_column %>% 
    gather(dates, value, -rowname) %>% 
    spread(rowname, value) %>%
    mutate(dates = ymd(dates),
          views = as.integer(`1`))

  slope <- ifelse(is.na(mean(foo$views)),0,summary(lm(views ~ dates, data = foo))$coef[2])
  slope_err <- ifelse(is.na(mean(foo$views)),0,summary(lm(views ~ dates, data = foo))$coef[4])

  bar <- tibble(
    rowname = rownr,
    min_view = min(foo$views),
    max_view = max(foo$views),
    mean_view = mean(foo$views),
    med_view = median(foo$views),
    sd_view = sd(foo$views),
    slope = slope/slope_err
  )

  return(bar)
}


```



```{r}

set.seed(1234)
foo <- sample_n(tpages, 5500) #5500
#foo <- tpages
rows <- foo$rowname
pcols <- c("rowname", "min_view", "max_view", "mean_view", "med_view", "sd_view", "slope")

params <- params_ts1(rows[1])

```


```{r}
for (i in seq(2,nrow(foo))){
  params <- full_join(params, params_ts1(rows[i]), by = pcols)
}

params <- params %>%
  filter(!is.na(mean_view)) %>%
  mutate(rowname = as.character(rowname))

```



### Overview visualisations

Exploring the parameter space we’ve built. (The global shape of the
distributions should not be affected by the sampling.) First we plot the
histograms of our main parameters:



```{r}
p1 <- params %>% 
  ggplot(aes(mean_view)) + geom_histogram(fill = "black", bins = 50) + scale_x_log10()
p2 <- params %>% 
  ggplot(aes(max_view)) + geom_histogram(fill = "black", bins = 50) + scale_x_log10()
p3 <- params %>% 
  ggplot(aes(sd_view/mean_view)) + geom_histogram(fill = "black", bins = 50) + scale_x_log10()
p4 <- params %>% 
  ggplot(aes(slope)) + geom_histogram(fill = "black", bins = 30) + 
  scale_x_continuous(limits = c(-25,25))

layout <- matrix(c(1,2,3,4),2,2,byrow=TRUE)
grid.arrange(p1, p2, p3, p4)
```


### Insights

* The distribution of average views is clearly bimodal, with peaks around 10
and 200-300 views. Something similar is true for the number of maximum 
views, although here the first peak (around 200) is curiuosly narrow. The 
second peak is centred above 10,000.

* The distribution of standard deviations (divided by the mean) is skewed 
toward higher values with larger numbers of spikes or stronger variability 
trends. Those will be the observations that are more challenging to 
forecast.

* The slope distribution is resonably symmetric and centred notably above zero.


Spliting the data by locale to focus on the densities:

```{r}

par_page <- left_join(params,tpages, by = "rowname")
p1 <- par_page %>% 
  ggplot(aes(mean_view, fill = locale)) +
  geom_density(position = "stack") +
  scale_x_log10(limits = c(1,1e4)) +
  theme(legend.position = "none")

p2 <- par_page %>% 
  ggplot(aes(max_view, fill = locale)) +
  geom_density(position = "stack") +
  scale_x_log10(limits = c(10,1e6)) +
  theme(legend.position = "none")

p3 <- par_page %>%
  ggplot(aes(sd_view, fill = locale)) +
  geom_density(position = "stack") +
  scale_x_log10(limits = c(1,1e5)) +
  theme(legend.position = "none")

p4 <- par_page %>% 
  ggplot(aes(slope, fill = locale)) +
  geom_density(position = "stack") + 
  scale_x_continuous(limits = c(-10,10))

layout <- matrix(c(1,2,3,4),2,2,byrow=TRUE)
grid.arrange(p1, p2, p3, p4)

```


### Insights

* The chinese pages (zh, in pink) are slightly but notably different from 
the rest. The have lower mean and max views and also less variation. Their 
slope distribution is broader, but also shifted more towards positive 
values compared to the other curves.

* The peak in max views around 200-300 is most pronounced in the french 
pages (fr, in turquoise).

* The english pages (en, in mustard) have the highest mean and maximum 
views, which is not surprising.




```{r}

params %>%
  ggplot(aes(max_view-mean_view, mean_view)) +
  geom_bin2d(bins = c(50,50)) +
  scale_x_log10() +
  scale_y_log10() +
  labs(x = "maximum views above mean", y = "mean views")

```



### Insights

* There is a clear correlation between mean views and maximum views. Also 
here we find again the two cluster peaks we had identified in the individual
histograms. A couple of outliers and outlier groups are noticeable.




Let’s zoom into the upper right corner (the numbers in parentheses are the row numbers):

```{r}

limx <- c(max(params$max_view)/35, max(params$max_view))
limy <- c(max(params$mean_view)/35, max(params$mean_view))
par_page %>%
  ggplot(aes(max_view-mean_view, mean_view)) +
  geom_point(size = 2, color = "red") +
  scale_x_log10(limits = limx) +
  scale_y_log10(limits = limy) +
  labs(x = "maximum views above mean", y = "mean views") +
  geom_label_repel(aes(label = str_c(article, " (",rowname,")")), alpha = 0.5)

```

## Individual observations with extreme parameters

Based on the overview parameters we can focus our attention on those
articles for which the time series parameters are at the extremes of the
parameter space.


### Large linear slope


Those are the observations with the highest slope values. (In the sample
this will be different, but in the full wikipedia data set the top 10 have
rownames 91728, 55587, 108341, 70772, 95367, 18357, 95229, 116150, 94975,
77292).

```{r}

params %>% arrange(desc(slope)) %>% head(5) %>% select(rowname, slope, everything())

```

Observing the time series data of the top 5 articles:

```{r}

p1 <- plot_rownr(91728)
p2 <- plot_rownr(55587)
p3 <- plot_rownr(108341)
p4 <- plot_rownr(70772)


layout <- matrix(c(1,2,3,4),2,2,byrow=TRUE)
grid.arrange(p1, p2, p3, p4)



```


### Insights

* Twenty One Pilots received a lot of views in Spain. 


Comparing the interest in Twenty One Pilots for  different countries, to see
whether a prediction for one of them could learn from the others:


```{r}

plot_names_nrm("Twenty_One_Pilots", "all-access", "all-agents")

```



### Insights

* Germany and France show quite similar viewing behaviour, while Russia and Spain are comparable too; especially in the early rise in interest. 
* The English pages show less dramatic changes, with the highest view count in the early years as compared to other regions.
* I think that the large spikes are close to impossible to predict. However, external data could help a lot here.



### Articles with decreasing view counts over the years



```{r}

params %>% arrange(slope) %>% head(5) %>% select(rowname, slope, everything())

```

Those are the observations with the lowest slope values. (In the sample
this will be different, but in the full wikipedia data set the Top 10: 95856, 74115, 8388, 103659, 100213, 9633, 102481 38458, 30042, 74002).



```{r}

p1 <- plot_rownr(95856)
p2 <- plot_rownr(74115)
p3 <- plot_rownr(8388)
p4 <- plot_rownr(103659)



layout <- matrix(c(1,2,3,4),2,2,byrow=TRUE)
grid.arrange(p1, p2, p3, p4)



```


### Insights

* The main page itself on mobile, and review articles on 2015 were the biggest losers.



## High standard deviations

```{r}

params %>% arrange(desc(sd_view/mean_view)) %>% head(5) %>% 
  select(rowname, sd_view, mean_view, max_view, everything())

```
Those are the observations with the highest standard deviation. (In the
sample this will be different, but in the full wikipedia data set the The 
top 10 wikipedia rows are 9775, 38574, 103124, 99323, 74115, 39181, 10404, 
33645, 34258, and 26994).



```{r}

p1 <- plot_rownr(9775)
p2 <- plot_rownr(38574)
p3 <- plot_rownr(103124)
p4 <- plot_rownr(99323)




layout <- matrix(c(1,2,3,4),2,2,byrow=TRUE)
grid.arrange(p1, p2, p3, p4)



```


### Insights

* 

## High average views

```{r}

params %>% arrange(desc(mean_view)) %>% 
  head(5) %>% select(rowname, max_view, mean_view, everything())

```

Those are the observations with the highest mean view count (In the
sample this will be different, but in the full wikipedia data set the The 
top 4 wikipedia rows are 38574, 9775, 74115, 139120).



```{r}

p1 <- plot_rownr(38574)
p2 <- plot_rownr(9775)
p3 <- plot_rownr(74115)
p4 <- plot_rownr(139120)


layout <- matrix(c(1,2,3,4),2,2,byrow=TRUE)
grid.arrange(p1, p2, p3, p4)



```


### Insights

* In addition to the spikes on the english main page there is a suprising
amount of variability as exemplified by the long-term structure in the 
German main page.




## Short-term variability






```{r}

p1 <- plot_rownr_zoom(10404, "2016-10-01", "2016-12-01")
p2 <- plot_rownr_zoom(9775, "2015-09-01", "2015-11-01")
p3 <- plot_rownr_zoom(139120, "2016-10-01", "2016-12-01")
p4 <- plot_rownr_zoom(110658, "2016-07-01", "2016-09-01")


layout <- matrix(c(1,2,3,4),2,2,byrow=TRUE)
grid.arrange(p1, p2, p3, p4)



```


### Insights

* We see that the high-view-count time series on the left hand side show a
very regular periodicity that is strikingly similar for both of them. 

* A similar structure can be seen on the right hand side, although here it 
is partly distorted by a slight upward trend (upper right) and/or variance 
caused by lower viewing numbers (lower right).

These plots provide evidence that there is variability on a weekly scale.

The next figure will visualise this *weekly behaviour* in a different way:

Here we average the variability in the previous plot over the day of the 
week and then overlay all four time series with different colours on a relative scale.

```{r}

rownr <- 10404
start <- "2016-10-01"
end <- "2016-12-01"
foo1 <- extract_ts(rownr) %>%
  filter(dates > ymd(start) & dates < ymd(end)) %>%
  mutate(dates = wday(dates, label = TRUE)) %>%
  group_by(dates) %>%
  summarise(wday_views = mean(views)) %>%
  mutate(wday_views = wday_views/mean(wday_views)) %>%
  mutate(id = factor(rownr))

rownr <- 9775
start <- "2015-09-01"
end <- "2015-11-01"
foo2 <- extract_ts(rownr) %>%
  filter(dates > ymd(start) & dates < ymd(end)) %>%
  mutate(dates = wday(dates, label = TRUE)) %>%
  group_by(dates) %>%
  summarise(wday_views = mean(views)) %>%
  mutate(wday_views = wday_views/mean(wday_views)) %>%
  mutate(id = factor(rownr))

rownr <- 139120
start <- "2016-10-01"
end <- "2016-12-01"
foo3 <- extract_ts(rownr) %>%
  filter(dates > ymd(start) & dates < ymd(end)) %>%
  mutate(dates = wday(dates, label = TRUE)) %>%
  group_by(dates) %>%
  summarise(wday_views = mean(views)) %>%
  mutate(wday_views = wday_views/mean(wday_views)) %>%
  mutate(id = factor(rownr))

rownr <- 110658
start <- "2016-07-01"
end <- "2016-09-01"
foo4 <- extract_ts(rownr) %>%
  filter(dates > ymd(start) & dates < ymd(end)) %>%
  mutate(dates = wday(dates, label = TRUE)) %>%
  group_by(dates) %>%
  summarise(wday_views = mean(views)) %>%
  mutate(wday_views = wday_views/mean(wday_views)) %>%
  mutate(id = factor(rownr))

foo <- bind_rows(foo1,foo2,foo3,foo4)

foo %>%
  ggplot(aes(dates, wday_views, color = id)) +
  geom_jitter(size = 4, width = 0.1) +
  labs(x = "Day of the week", y = "Relative average views")

```



### Insights

*  We can observe a clear trend toward lower viewing numbers on the weekend 
(Fri/Sat/Sun), and also a declining trend from Monday through Thursday. 

* This gives us valuable information on the general type of variability over
the course of a week. In order to study this behaviour more in detail, we 
would need to average over a larger number of time series.








